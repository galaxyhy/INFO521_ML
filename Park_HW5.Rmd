---
title:  'Homework 5'
subtitle: 'ISTA 421/INFO 521'
author:
- name: Student -  Jung Mee Park
  affiliation: University of Arizona
- name: Instructor -  Cristian Roman-Palacios
  affiliation: School of Information, University of Arizona, Tucson, AZ
tags: [R, RStudio, HW4]
output: html_document
---

---------------

### Objectives
This homework sheet will help reviewing the basic concepts associated with model selection and regularization. Please review the lectures, suggested readings, and additional resources _before_ getting started on the HW.

---------------

#### Additional resources relevant to this HW

- __R Markdown__: Please review the basic R Markdown cheat sheet in case you have any questions regarding formatting the HW: https://www.rstudio.com/wp-content/uploads/2015/02/rmarkdown-cheatsheet.pdf.

- __R__: Please review the basic R` cheat sheet in case you have any questions regarding the programming language: https://www.soa.org/globalassets/assets/Files/Edu/2018/exam-pa-base-r.pdf.

- __RStudio__: Additional cheat sheets written by RStudio to help with specific `R` packages: https://www.rstudio.com/resources/cheatsheets/

- __Datasets__: The following website has access to the relevant datasets from the recommended textbook: https://book.huihoo.com/introduction-to-statistical-learning/data.html


#### The *Tidyverse*

I encourage students to check out functions from packages included in the `tidyverse` (https://www.tidyverse.org/) which greatly facilitates the productivity of novice coders. However, all instructions will be delivered using `Base` `R`. The main textbook also uses base `R` only. I will be happy to grade your code regardless whether it uses base `R` or functions in the `tidyverse.` For some steps, other packages (such as `data.table`) are an appropriate alternative. Most if not all the questions in this HW can be answered using the tidyverse. Please check out the “accompanying” book to our main textbook that uses packages from the tidyverse instead of base (https://emilhvitfeldt.github.io/ISLR-tidymodels-labs/index.html). For this chapter, please follow the instructions in the following site: https://emilhvitfeldt.github.io/ISLR-tidymodels-labs/linear-regression.html. 



### Scores

Please answer the questions from the section that you’re enrolled in (labeled as either __421__ or __521__). Below is a summary of the total scores associated with this HW (2 points per question).


- __ISTA 421__: 8 points (undergraduate)
- __INFO 521__: 10 points (graduate)
- __Extra credit__: 3 points


### Submission:
Please follow the instructions outlined below to submit your assignment. __This HW is due at the end of the same week that is released (Sunday, 11:59 pm AZ time)__. Please get in touch with the instructor if you’re (i) having issues opening the assignment, (2) not understanding the questions or (3) having issues submitting your assignment. Note that late submissions are subject to a penalty (see late work policies in the Syllabus).


-**Homework 5**: Please turn in a __single RMD file (this file) AND a rendered HTML__. Answers to each question should be in the relevant block of code (see below). Re-name your file to __lastname_Hw5.RMD__ before submitting. __Make sure that you can correctly render (knit) your submission without errors before turning anything in__. If a given block of code is causing you issues and you didn’t get to fix it on time, please use `eval=FALSE` in the relevant block. If you’re using additional files to render your __RMD__, please include each of them in your submission.


### Time commitment
Please do reach out if you’re taking more than ~18h to complete (1) this HW, (2) reading the assigned book chapters, and (3) going over the lectures. I will be happy to provide accommodations if necessary. Do not wait until the last minute to start working on the HW. In most cases, working under pressure will certainly increase the time needed to answer each of these questions. The instructor might not be 100% available on Sundays to troubleshoot with you. Remember that you can sign up for office hours with the instructor 3 times a week.

### Looking for help?
First, please go over the relevant readings for this week. Second, if you’re still struggling with any of the questions, do some independent research (e.g. stackoverflow is a wonderful resource). __Don’t forget that your classmates will also be working on the same questions - reach out for help (check the Discussion forum in D2L for advice from other students)__. Finally, the instructor will be happy to answer any questions during office hours. You can reach out to me by email (cromanpa94@arizona.edu) or simply schedule a 15 minute meeting through Calendly (https://calendly.com/cromanpa/15min)*

Please do not forget that the instructor holds office hours 3 times a week!!


### Grading
_Please note that grades are NOT exclusively based on your final answers_. I will be grading the overall structure and logic of your code. Feel free to use as many lines as you need to answer each of the questions. I also highly recommend and strongly encourage adding comments (`#`) to your code. Comments will certainly improve the reproducibility and readability of your submission. Commenting your code is also good coding practice. Specifically for the course, you’ll get better feedback if the instructor is able to understand your code in detail.


---------------

# Questions

This homework is divided into two main parts. First, a conceptual component will review the basic concepts related to resampling. The second part of the homework is mostly intended to be a brief introduction to regularization methods and resampling in `R`. Several of these questions are modified from James et al. (2021).


## Conceptual

#### Question 1 (421/521)

We will derive the probability that a given observation is part of a bootstrap sample. Suppose that we obtain a bootstrap sample from a set of `n` observations. **Don’t forget that samples are obtained with replacement**.

  a) What is the probability that the first bootstrap observation is not the $jth$ observation from the original sample? Justify your answer.

> **_Answer:_**  [BEGIN SOLUTION].
Because there are n number of observations with each observation having an equal chance of being selected. The probability of the jth observation being selected is 1/n. Therefore the probability of it not being the jth is 1-(1/n).

  b) What is the probability that the second bootstrap observation is not the $jth$ observation from the original sample?

> **_Answer:_**  [BEGIN SOLUTION].
In instances of sampling with replacement, the probability would be the same as the first bootstrap of 1-(1/n).

  c) Argue that the probability that the $jth$ observation is not in the bootstrap sample is $(1 − 1/n)^n$.

> **_Answer:_**  [BEGIN SOLUTION].
Sampling n times, randomly and with replacement, would be similar to the first bootstrap of what the jth is not (1-(1/n)). The samples are independent, thus the probabilities can be mulpitied where the jth is not in (1-(1/n))^n. 

  d) When `n = 5`, what is the probability that the $jth$ observation is in the bootstrap sample?

> **_Answer:_**  [BEGIN SOLUTION].
If there is only an n=5. The probability will be 1-(1-(1/5))^5, which produces a result of 0.67232.

  e) When `n = 100`, what is the probability that the $jth$ observation is in the bootstrap sample?

> **_Answer:_**  [BEGIN SOLUTION].
1-(1-(1/100))^100 is 0.634.

  f) When `n = 1000`, what is the probability that the $jth$ observation is in the bootstrap sample?

> **_Answer:_**  [BEGIN SOLUTION].
1-(1-(1/1000))^1000 is 0.6323.



#### Question 2 (421/521)


The following questions are relative to k-fold cross-validation.

  a) Explain how k-fold cross-validation is implemented.
  
> **_Answer:_**  [BEGIN SOLUTION].
Each fold represents a segment. The k groups or folds are distinct. The model is trained using k-1 and the tested on the subsequent folds. The training happens k times. The MSE or something like it is averaged to give a cross -validation measure. 

  b) How would you choose the analyzed k? Does that matter? (please cite your sources)

> **_Answer:_**  [BEGIN SOLUTION].
A common approach is to pick k=5 or k=10 for  computational ease (James et al, 2021: 203). Rather than choosing k=n, k=10 for a cross-validition performs the fit only 10 times (James et al, 2021: 204).

  c) What are the advantages and disadvantages of k-fold cross-validation relative to: *The validation set* approach?

> **_Answer:_**  [BEGIN SOLUTION].

**advantages:** The entire dataset is used. Cross validation works well for small datasets due to low variability. CV is easier to understand conceptually.

**disadvantages:** CV may over estimate the test error but can be corrected only fit over the entire dataset. 

  d) What are the advantages and disadvantages of k-fold cross-validation relative to: *LOOCV*?

> **_Answer:_**  [BEGIN SOLUTION].

**advantages:** CV scales better. When the n is greater than 10, LOOCV would be taxed to run many more iterations than if we stuck with CV. Even though the bias may be higher, variance is low for CV. The reverse is true for LOOCV. This is known as the bias-variance trade-off (James et al, 2021: 205).

**disadvantages:** Based on how the folds are made, the CV method is subject to much randomness. LOOCV could also be less computationally intensive in cases where the n is small.

## Applied

Please note that some of the questions outlined below make suggestions on potential functions to be used in the answers. Unless indicated otherwise, feel free to use any other function. For those interested in improving their R skills, I would recommend going over the information in the following book for _tidyverse_ (https://emilhvitfeldt.github.io/ISLR-tidymodels-labs/linear-regression.html), using _caret_ to answer some of the questions (https://topepo.github.io/caret/), or _mikropml_ (https://cran.r-project.org/web/packages/mikropml/vignettes/introduction.html)

#### Question 3 (421/521)

```{r message=FALSE}
#load libraries
library(MASS)
library(caret)
library(ISLR2)
library(glmnet)
library(leaps)
library(tidyverse)
```

  a) What is the following code supposed to do? 

```{r}
# Simulate some data
n <- 100
order <- 2
x <- runif(n, -4, 4)
y = rnorm(n = n, mean = x^order, sd = 0.25)
data <- data.frame(x=x, y=y)

# Number of folds 
n.folds <- 20 #(K)

# Polynomial orders
poly.degree <- seq(1, 10) 

# List of indices per fold
indices <- split(seq(n), sort(seq(n) %% n.folds))
mse.estimates <- c()
for(q in poly.degree){
  y.hat <- c()
  for(fold.idx in indices) {
    # Fit on K-1 folds
    ols <- lm(y~poly(x,q), data=data[-fold.idx,])
    # Predict on the kth fold
    y.hat[fold.idx] <- predict(ols, newdata = data[fold.idx,])
  }
  # Concatenate all predictions
  y.hat <- unname(unlist(y.hat))
  # Get the mse estimate
  mse.estimates[q] <- mean((y.hat - data[, 'y'])^2)
}
```


> **_Answer:_**  [BEGIN SOLUTION].
This could be for k-folds cross validation. The first chunk of the code creates a random sample of 100 variables x and y. The y has a standard deviation of .25 and the mean is x to the order of 2. The two variables are bound to create a dataset. n equals the number of folds which is 20 in this case. 

  b) To the best of your knowledge, improve the structure, content, clarity, and reproducibility of the code presented before in part a) of this question (e.g. would you run a single or multiple replicates?). Fix any mistakes (if you find any). Finally, generate at least two plots summarizing your findings regarding the best-fitting polynomial order on the simulated dataset (also from part a): (1) MSE vs polynomial order, and (2) x vs y, along with a plot of the selected model.
  
  
```{r}
# BEGIN SOLUTION
# consider looking at training and testing data

# Number of folds 
n.folds <- 10 #(K)

# Polynomial orders
poly.degree <- seq(1, 10) # polynomial degrees to loop over to select

# List of indices per fold
# q is the polynomial degrees
indices <- split(seq(n), sort(seq(n) %% n.folds))
mse.estimates <- c()
for(q in poly.degree){
  y.hat <- c()
  for(fold.idx in indices) {
    # Fit on K-1 folds
    ols <- lm(y~poly(x,q), data=data[-fold.idx,])
    # Predict on the kth fold
    # to get cross-validated precitions
    y.hat[fold.idx] <- predict(ols, newdata=data[fold.idx,])
  }
  # Concatenate all predictions
  y.hat <- unname(unlist(y.hat))
  # Get the mse estimate
  mse.estimates[q] <- mean((y.hat - data[, 'y'])^2)
}

# par(mfrow=c(1,2)) 
plot(poly.degree, mse.estimates, main="MSE estimates (K-fold cross-validation)", xlab="Polynomial degree", ylab="MSE estimate", type="l", lty=2, col=2)
which.min(mse.estimates) # 2
# 

xy_plot <- ggplot(data=data, aes(x = x, y = y)) +
  # facet_wrap(~ z, nrow=1, ncol=2) +
  geom_line(color="red") + 
  geom_jitter() +
  geom_smooth(method = loess, formula =   
                lm(y~poly(x,q),data=data[-fold.idx,]), span = 0.3) + 
  ggtitle('selected model') 
  # geom_smooth(method = lm, formula = lm(y~poly(x,q), data=data[-fold.idx,]), se=TRUE, span = 0.3) 
xy_plot
# END SOLUTION
```


#### Question 4 (421/521)

Bootstrap the following dataset (`n = 1000`) to obtain median and 95% CI for parameter estimates (slope and intercept) summarizing the relationship between `x` and `y_measured.` What happens with median parameter estimates when you examine `y` instead? Your solution **must** use `for` and `sample` Provide some visualizations to explain your results.

```{r}
set.seed(1)
nobs <- 100
x1 <- rnorm(nobs)
y1 <- 5-2.25*x1 
y_measured <- y1 + rnorm(nobs)

# BEGIN SOLUTION
# full data set 
data2 <- cbind.data.frame(x1, y1, y_measured)

# one model has error, one model does not
# for loop
# sample with replacement from data2
# save the coef 1000 times
sample_coef_intercept <- NULL
sample_coef_x1 <- NULL

# for y_measured
for (i in 1:1000) {
  #Creating a resampled dataset from data2
  sample_d = data2[sample(1:nrow(data2), nrow(data2), replace = TRUE), ]
  #Running the regression on these data
  model_bootstrap <- lm(y_measured ~ x1, data = sample_d)
  #Saving the coefficients
  sample_coef_intercept <-
    c(sample_coef_intercept, model_bootstrap$coefficients[1])
  
  sample_coef_x1 <-
    c(sample_coef_x1, model_bootstrap$coefficients[2])
}
coefs <- rbind(sample_coef_intercept, sample_coef_x1)

# confidence interval at 95% with y_measured
a <-
  cbind(
    quantile(sample_coef_intercept, prob = 0.025),
    quantile(sample_coef_intercept, prob = 0.975))
a
b <-
  cbind(quantile(sample_coef_x1, prob = 0.025),
        quantile(sample_coef_x1, prob = 0.975))
b

# median parameter estimates for y 
median.boot <- c(median(sample_coef_intercept), median(sample_coef_x1))
median.boot 

# plotting for y_measured 
xy_measured_plot <- ggplot(data2, aes(x=x1, y=y_measured)) +
  geom_point() + 
  geom_smooth(method = "lm") +
  ggtitle("Plot for X and Y-measured")
xy_measured_plot

# for y 
sample_coef_intercept1 <- NULL
sample_coef_x1_for_y <- NULL
for (i in 1:1000) {
  #Creating a resampled dataset from data2
  sample_d = data2[sample(1:nrow(data2), nrow(data2), replace = TRUE), ]
  #Running the regression on these data
  model_bootstrap1 <- lm(y1 ~ x1, data = sample_d)
  #Saving the coefficients
  sample_coef_intercept1 <-
    c(sample_coef_intercept1, model_bootstrap1$coefficients[1])
  
  sample_coef_x1_for_y <-
    c(sample_coef_x1_for_y, model_bootstrap1$coefficients[2])
}
coefs1 <- rbind(sample_coef_intercept1, sample_coef_x1_for_y)

# confidence interval at 95% with y
c <-
  cbind(
    quantile(sample_coef_intercept1, prob = 0.025),
    quantile(sample_coef_intercept1, prob = 0.975))
c
d <-
  cbind(quantile(sample_coef_x1_for_y, prob = 0.025),
        quantile(sample_coef_x1_for_y, prob = 0.975))
d

# median parameter estimates for y 
median.boot1 <- c(median(sample_coef_intercept1), median(sample_coef_x1_for_y))
median.boot1 

# plotting for y
xy_plot <- ggplot(data2, aes(x=x1, y=y1)) +
  geom_point() +
  geom_smooth(method = "lm") +
  ggtitle("Plot for X and Y")
xy_plot

# END SOLUTION
```

The plot of x and y show a straight line. The plot of x and y_measured had more values that deviated from the line.  
#### Question 5 (521)

We will predict the number of applications received using the other variables in the College data set. Please load the relevant dataset first.

  a) Split the data set into a training set and a test set.

```{r}
# BEGIN SOLUTION
College <- read.csv("https://book.huihoo.com/introduction-to-statistical-learning/College.csv")
# remove missing rows with missing variables
College <- na.omit(College)
College <- College[,-1]
# College$Elite <- ifelse(College$Top10perc >= 50, TRUE, FALSE)

sample_size <- round(nrow(College)*.70) # setting what is 70%
index <- sample(seq_len(nrow(College)), size = sample_size)
 
train.College <- College[index, ]
test.College <- College[-index, ]
# END SOLUTION
```

  b) Fit a linear model using least squares on the training set, and report the test error obtained.

```{r}
# BEGIN SOLUTION
college.lm <- lm(Apps ~ ., data = train.College)
summary(college.lm) # residual standard error is 13.65

y_hat_train <- predict(college.lm, data = train.College)
train_MSE <- mean((train.College$Apps - y_hat_train)^2)

ggplot(train.College, aes(x = Apps, y=Top10perc)) +
    geom_point() + 
    geom_smooth(method = "lm") +
    scale_y_continuous(limits = c(0, 100)) 
  
# coef(college.lm)
# summary(college.lm)$coef
# 
# college.lm.pred <- predict(college.lm, test.College)
# college.lm.class <- college.lm.pred$
# table(college.lm.class, test.College$Grad.Rate)
# mean(college.lda.class != test.College$Grad.Rate)
# END SOLUTION
```

  c) Fit a ridge regression model on the training set, with $\lambda$ chosen by cross-validation. Report the test error obtained.
  
  
```{r}
# BEGIN SOLUTION
# choosing a model using CV

# # ridge regression
matrix_mod <- model.matrix(Apps ~ ., data = train.College)

grid <- 10^seq(10, -2, length=100)
ridge.mod <- glmnet(matrix_mod, train.College$Apps, alpha = 0, lambda = grid)

dim(coef(ridge.mod))
plot(ridge.mod)    # Draw plot of coefficients

ridge.mod$lambda[50] #Display 50th lambda value
coef(ridge.mod)[,50] # Display coefficients associated with 50th lambda value
sqrt(sum(coef(ridge.mod)[-1,50]^2)) # Calculate l2 norm

# train and testing models to calculate sample errors
x_train = model.matrix(Apps~., train.College)[,-1]
x_test = model.matrix(Apps~., test.College)[,-1]

# ridge_mod with all the variables
ridge_mod <- glmnet(x_train, train.College$Apps, alpha=0, lambda = grid, thresh = 1e-12)
ridge_pred = predict(ridge_mod, s = 4, newx = x_test)

# for test MSE
mean((ridge_pred - test.College$Apps)^2) # the test MSE is 912817

# # for training MSE
# mean((mean(train.College$Apps) - test.College$Apps)^2) # 9970586
#
# # to find a better lambda
# # ridge_pred = predict(ridge_mod, s = 0, newx = x_test, exact = T)
# mean((ridge_pred - test.College$Apps)^2)

# using CV to find the best lambda
set.seed(1)
cv.out = cv.glmnet(x_train, train.College$Apps, alpha = 0) # Fit ridge regression model on training data
bestlam = cv.out$lambda.min  # Select lamda that minimizes training MSE
bestlam #322

# plot to see the smallest lamda
plot(cv.out)

ridge_pred = predict(ridge_mod, s = bestlam, newx = x_test) # Use best lambda to predict test data
mean((ridge_pred - test.College$Apps)^2) # Calculate test MSE which was 3197161

# out <- glmnet(ridge.mod, train.College$Apps, alpha = 0) # Fit ridge regression on a small model
# predict(out, type = "coefficients", s = bestlam) # Display coefficients using lambda chosen by CV

# another approach
#define response variable
y2 <- train.College$Apps
#define matrix of predictor variables
x2 <- data.matrix(train.College[, c("Private", "Accept", "Top10perc", "Top25perc", "Outstate", "Room.Board", "Expend", "Grad.Rate")])

#fit ridge regression model
model <- glmnet(x2, y2, alpha = 0)
#view summary of model
summary(model)

#perform k-fold cross-validation to find optimal lambda value
cv_model <- cv.glmnet(x2, y2, alpha = 0)

#find optimal lambda value that minimizes test MSE
best_lambda <- cv_model$lambda.min
best_lambda

#produce plot of test MSE by lambda value
plot(cv_model)

#find coefficients of best model
best_model <- glmnet(x2, y2, alpha = 0, lambda = best_lambda)
coef(best_model)

#produce Ridge trace plot
plot(model, xvar = "lambda")

#use fitted best model to make predictions
y_predicted <- predict(model, s = best_lambda, newx = x2)

#find SST and SSE
sst <- sum((y2 - mean(y2))^2)
sse <- sum((y_predicted - y2)^2)
sse
#find R-Squared
rsq <- 1 - sse/sst
rsq

# END SOLUTION
```


  d) Fit a lasso model on the training set, with $\lambda$ chosen by cross-validation. Report the test error obtained, along with the number of non-zero coefficient estimates.

```{r}
# BEGIN SOLUTION
lasso_mod <- glmnet(x_train, 
                   train.College$Apps, 
                   alpha = 1, 
                   lambda = grid) # Fit lasso model on training data

# Draw plot of coefficients
plot(lasso_mod)    

# cross-validation and compute the associated test error
set.seed(1)
cv.out <- cv.glmnet(x_train, train.College$Apps, alpha = 1) # Fit lasso model on training data
plot(cv.out) # Draw plot of training MSE as a function of lambda
bestlam = cv.out$lambda.min # Select lamda that minimizes training MSE
lasso_pred = predict(lasso_mod, s = bestlam, newx = x_test) # Use best lambda to predict test data
mean((lasso_pred - test.College$Apps)^2) # Calculate test MSE

out <- glmnet(matrix_mod, train.College$Apps, alpha = 1, lambda = grid) # Fit lasso model on full dataset
lasso_coef <- predict(out, type = "coefficients", s = bestlam) # Display coefficients using lambda chosen by CV
lasso_coef

lasso_coef[lasso_coef != 0] # Display only non-zero coefficients

# # another approach 
# #perform k-fold cross-validation to find optimal lambda value
# cv_model_lasso <- cv.glmnet(x2, y2, alpha = 1)
# 
# #find optimal lambda value that minimizes test MSE
# best_lambda_lasso <- cv_model_lasso$lambda.min
# best_lambda_lasso
# 
# [1] 5.616345
# 
# #produce plot of test MSE by lambda value
# plot(cv_model) 
# 
# #find coefficients of best model
# best_model_lasso <- glmnet(x2, y2, alpha = 1, lambda = best_lambda_lasso)
# coef(best_model_lasso)
# 
# #use fitted best model to make predictions
# y_predicted_lasso <- predict(best_model_lasso, s = best_lambda_lasso, newx = x2)
# 
# #find SST and SSE
# sst_lasso <- sum((y2 - mean(y2))^2)
# sse_lasso <- sum((y_predicted_lasso - y2)^2)
# 
# #find R-Squared
# rsq_lasso <- 1 - sse_lasso/sst_lasso
# rsq_lasso

# END SOLUTION
```

So there may be 17 non-zero coefficients. 

## Extra credit

What is the best polynomial order (1–20) modeling the structure of the following dataset (`x` vs `y`)? Your answer should include a numerical approach (e.g. MSE estimates) along with a convincing set of visuals supporting your answer.

```{r}
set.seed(4)
n <- 1000
x <- runif(n, -4, 4)
mu <- ifelse(x > 0, cos(4*x), 1-sin(x))
y <- mu + rnorm(n)
data <- data.frame(x=x, y=y)
plot(x, y, col="black",  pch=21, bg="steelblue", ylab = "y")
lines(x[order(x)], mu[order(x)], col="red", lwd=3, type="l")
legend("bottomleft", legend=c("E[Y|X=x], truth", "Data"), cex=.8, lty=c(1, NA), col=c("red","steelblue"),  pch=c(NA, 21), pt.bg=c(NA, "steelblue"))
```

# Look at code in Question 3

```{r}
# CODE from Qu3. Simulate some data
# n <- 100
# order <- 2
# x <- runif(n, -4, 4)
# y = rnorm(n = n, mean = x^order, sd = 0.25)
# data <- data.frame(x=x, y=y)

# Number of folds 
n.folds1 <- 20 #(K)

# Polynomial orders
poly.degree1 <- seq(1, 10) 

# List of indices per fold
indices <- split(seq(n), sort(seq(n) %% n.folds1))
mse.estimates1 <- c()
for(q in poly.degree1){
  y.hat <- c()
  for(fold.idx in indices) {
    # Fit on K-1 folds
    ols <- lm(y~poly(x,q), data=data[-fold.idx,])
    # Predict on the kth fold
    y.hat[fold.idx] <- predict(ols, newdata = data[fold.idx,])
  }
  # Concatenate all predictions
  y.hat <- unname(unlist(y.hat))
  # Get the mse estimate
  mse.estimates1[q] <- mean((y.hat - data[, 'y'])^2)
}

plot(poly.degree1, mse.estimates1, main="MSE estimates (K-fold cross-validation)", xlab="Polynomial degree", ylab="MSE estimate", type="l", lty=2, col=2)
which.min(mse.estimates) # 10
# 
```

