---
title:  'Homework 3'
subtitle: 'ISTA 421/INFO 521'
author:
- name: Jung Mee Park
  affiliation: UITS, University of Arizona 
- name: Instructor -  Cristian Roman-Palacios
  affiliation: School of Information, University of Arizona, Tucson, AZ
tags: [R, RStudio, HW3]
output: html_document
---

---------------

### Objectives
This homework sheet will help reviewing the basic concepts associated with linear regression models. Please review the lectures, suggested readings, and additional resources __before__ getting started on the HW.

---------------

#### Additional resources relevant to this HW

- __R Markdown__: Please review the basic R Markdown cheat sheet in case you have any questions regarding formatting the HW: https://www.rstudio.com/wp-content/uploads/2015/02/rmarkdown-cheatsheet.pdf.

- __R__: Please review the basic R` cheat sheet in case you have any questions regarding the programming language: https://www.soa.org/globalassets/assets/Files/Edu/2018/exam-pa-base-r.pdf.

- __RStudio__: Additional cheat sheets written by RStudio to help with specific `R` packages: https://www.rstudio.com/resources/cheatsheets/

- __Datasets__: The following website has access to the relevant datasets from the recommended textbook: https://book.huihoo.com/introduction-to-statistical-learning/data.html


#### The *Tidyverse*

I encourage students to check out functions from packages included in the `tidyverse` (https://www.tidyverse.org/) which greatly facilitates the productivity of novice coders. However, all instructions will be delivered using `Base` `R`. The main textbook also uses base `R` only. I will be happy to grade your code regardless whether it uses base `R` or functions in the `tidyverse.` For some steps, other packages (such as `data.table`) are an appropriate alternative. Most if not all the questions in this HW can be answered using the tidyverse. Please check out the “accompanying” book to our main textbook that uses packages from the tidyverse instead of base (https://emilhvitfeldt.github.io/ISLR-tidymodels-labs/index.html). For this chapter, please follow the instructions in the following site: https://emilhvitfeldt.github.io/ISLR-tidymodels-labs/linear-regression.html. 



### Scores

Please answer the questions from the section that you’re enrolled in (labeled as either __421__ or __521__). Below is a summary of the total scores associated with this HW (non-extra credit: 2 points per question; extra credit: 5 points per question).


- __ISTA 421__: 14 points (undergraduate)
- __INFO 521__: 16 points (graduate)
- __Extra credit__: 10 points (ISTA421/INFO521).


### Submission:
Please follow the instructions outlined below to submit your assignment. __This HW is due at the end of the same week that is released (Sunday, 11:59 pm AZ time)__. Please get in touch with the instructor if you’re (i) having issues opening the assignment, (2) not understanding the questions or (3) having issues submitting your assignment. Note that late submissions are subject to a penalty (see late work policies in the Syllabus).


-**Homework 3**: Please turn in a __single RMD file (this file) AND a rendered HTML__. Answers to each question should be in the relevant block of code (see below). Re-name your file to __lastname_Hw3.RMD__ before submitting. __Make sure that you can correctly render (knit) your submission without errors before turning anything in__. If a given block of code is causing you issues and you didn’t get to fix it on time, please use `eval=FALSE` in the relevant block. If you’re using additional files to render your __RMD__, please include each of them in your submission.


### Time commitment
Please do reach out if you’re taking more than ~18h to complete (1) this HW, (2) reading the assigned book chapters, and (3) going over the lectures. I will be happy to provide accommodations if necessary. Do not wait until the last minute to start working on the HW. In most cases, working under pressure will certainly increase the time needed to answer each of these questions. The instructor might not be 100% available on Sundays to troubleshoot with you. Remember that you can sign up for office hours with the instructor 3 times a week.

### Looking for help?
First, please go over the relevant readings for this week. Second, if you’re still struggling with any of the questions, do some independent research (e.g. stackoverflow is a wonderful resource). __Don’t forget that your classmates will also be working on the same questions - reach out for help (check the Discussion forum in D2L for advice from other students)__. Finally, the instructor will be happy to answer any questions during office hours. You can reach out to me by email (cromanpa94@arizona.edu) or simply schedule a 15 minute meeting through Calendly (https://calendly.com/cromanpa/15min)*

Please do not forget that the instructor holds office hours 3 times a week!!


### Grading
_Please note that grades are NOT exclusively based on your final answers_. I will be grading the overall structure and logic of your code. Feel free to use as many lines as you need to answer each of the questions. I also highly recommend and strongly encourage adding comments (`#`) to your code. Comments will certainly improve the reproducibility and readability of your submission. Commenting your code is also good coding practice. Specifically for the course, you’ll get better feedback if the instructor is able to understand your code in detail.


---------------

# Questions

This homework is divided into two main parts. First, a conceptual component will review the basic concepts related to linear models. The second part of the homework is mostly intended to be a brief introduction to fitting and analyzing linear models in R. Several of these questions are modified from James et al. (2021).


## Conceptual

#### Question 1 (421/521)

Describe the null hypotheses to which the _p_-values given in the table (see below). Explain what conclusions you can draw based on these p-values. Your explanation should be phrased in terms of sales, TV, radio, and newspaper, rather than in terms of the coefficients of the linear model.

|  | Coefficient | Std. error | t-statistics | p-value |
|---|:---:|:---:|:---:|:---:|
| Intercept | 2.939 | 0.3119 | 9.42 | <0.0001 |
| TV | 0.046 | 0.0014 | 32.81 | <0.0001 |
| Radio | 0.189 | 0.0086 | 21.89 | <0.0001 |
| Newspaper | -0.001 | 0.0059 | -0.18 | 0.8599 |

> **_Answer:_**  [BEGIN SOLUTION].
TV and radio ads have an impact on Sales, whereas the p-value for Newspaper is so large that we can assume that newspaper ads have no effect on sales. A small p-value indicates that the relationship was unlikely due to chance. We can reject the null hypothesis since some ads do have an impact on sales. 

#### Question 2 (421/521)

Assume that you collected a dataset of 100 observations containing a single predictor and a quantitative response. Then, you decided to fit a (i) linear regression model to the data, as well as a separate (ii) cubic regression, i.e. $Y = β_{0} + β_{1}X^{1} +β_{1}X{1}.. + ϵ$.

a) Suppose that the true relationship between $X$ and $Y$ is linear, i.e. $Y = β_{0} + β_{1}X + ϵ$. Consider the training residual sum of squares (_RSS_) for the linear regression, and also the training _RSS_ for the cubic regression. Would we expect one to be lower than the other, would we expect them to be the same, or is there not enough information to tell? Justify your answer.

> **_Answer:_**  [BEGIN SOLUTION].
RSS would decrease as model flexibility increases as in the cubic model. 

b) Answer part (a) using the test rather than training RSS.
RSS will probably be higher for the test RSS because the underlying model is linear. 

> **_Answer:_**  [BEGIN SOLUTION].

c) Suppose that the true relationship between $X$ and $Y$ is not linear. However, we don’t know how far it is from linear. Consider the training _RSS_ for the linear regression, and also the training _RSS_ for the cubic regression. Would we expect one to be lower than the other, would we expect them to be the same, or is there not enough information to tell? Justify your answer.

> **_Answer:_**  [BEGIN SOLUTION].
Flexible model will allow the model to fit more closely . 

d) Answer (c) using test rather than training _RSS._

> **_Answer:_**  [BEGIN SOLUTION].
Because the true relationship is unknown, the training dataset may have overfit the model leading to a higher RSS in the testig data. 

#### Question 3 (421/521)

a) Discuss the implications that uncertainty in $X$ would have on parameter estimates based on linear regression models. Should we care about error in $X$ when fitting OLS regressions? 

> **_Answer:_**  [BEGIN SOLUTION].

b) Provide a brief discussion on the concept of regression dilution.

> **_Answer:_**  [BEGIN SOLUTION].
Also known as a regression attenuation, regression diluation biases slope in regressions towards 0. This may underestimate the true values and cause errors in the independent variable. 

c) Are there any linear regression models that account for uncertainty in $X$ and $Y$? Please do some independent research and cite your sources.

> **_Answer:_**  [BEGIN SOLUTION].
Bayesian factors for GLM's (Raftery, 1996) and Bayesian bootstrapping (Wang et al 2015) make some considerations for model uncertainty, especially in generalised models. 

Raftery, A. (1996). Approximate Bayes factors and accounting for model uncertainty in generalised linear models. Biometrika, 83(2), 251–266. https://doi.org/10.1093/biomet/83.2.251

Wang, C., Dominici, F., Parmigiani, G., & Zigler, C. M. (2015). Accounting for uncertainty in confounder and effect modifier selection when estimating average causal effects in generalized linear models. Biometrics, 71(3), 654–665. https://doi.org/10.1111/biom.12315


d) Distinguish between least squares and maximum likelihood in the context of linear regression models.

> **_Answer:_**  [BEGIN SOLUTION].
Given a scatterplot with normally distributed errors, a linear line can be drawn. The point from the model line to the true value is a distance that can be summed up and squared. The Maximum likelihood estimators can be the same for the intercept and coefficients. Maximum likelihood chooses estimates that fits the parameters of the sample data. 

## Applied

Please note that some of the questions outlined below make suggestions on potential functions to be used in the answers. Feel free to use any other function. These are just a reference. For those interested in improving their skills, I would recommend going over the information in the following book for _tidyverse_ (https://emilhvitfeldt.github.io/ISLR-tidymodels-labs/linear-regression.html), using _caret_ to answer some of the questions (https://topepo.github.io/caret/), or _mikropml_ (https://cran.r-project.org/web/packages/mikropml/vignettes/introduction.html)

#### Question 4

This question involves the use of simple linear regression on the Auto data set.

a) Use the `lm()` function to perform a simple linear regression with mpg as the response and horsepower as the predictor. Use the `summary()` function to print the results. Then, answer the questions below:

```{r}
# BEGIN SOLUTION
library(datasets)
data("mtcars")

mpg_lm <- lm(mpg ~ hp, data = mtcars)
summary(mpg_lm)
# END SOLUTION
```


i. Is there a relationship between the predictor and the response?

> **_Answer:_**  [BEGIN SOLUTION].
Yes. It appears to be a negative relationship.

ii. How strong is the relationship between the predictor and the response? Focus on the _RSE_, estimated $R^2$, and _slope._

> **_Answer:_**  [BEGIN SOLUTION].
The relationship is fairly strong. The adjusted R-sq is only 0.59, which means only some of variation in mpg is explained through the horsepower predictor.  

iii. Is the relationship between the predictor and the response positive or negative?

> **_Answer:_**  [BEGIN SOLUTION].
It is negative in that as HP increases the mpg decreases.

iv. What is the predicted mpg associated with a __horsepower__ of __98__? What are the associated 95% confidence and prediction intervals? 
- Looking at page 83 in James et al, 2021


```{r}
# BEGIN SOLUTION
confint(mpg_lm)
predict(mpg_lm, data.frame(hp = (c(98))), interval = "prediction")
        
# END SOLUTION
```

v. Now, please briefly explain the main use of inverse predictions. List at least two practical examples.

> **_Answer:_**  [BEGIN SOLUTION].
The inverse prediction can be used to check the model fit, especially if a graph was made and fitted to a line. If the model gives us a result that is not similar to the plot, then we have to adjust the models. Sometimes you may not have a lot of data points, so to create a graph, you may need to generate some observations. 

vi. Finally, use `chemCal::inverse.predict` (or any function of your preference) to infer horsepower from the same mpg value predicted in (__iv__). Comment on your results.

```{r}
# BEGIN SOLUTION
library(chemCal)
mpg_lm <- lm(mpg ~ hp, data = mtcars)
chemCal::inverse.predict(mpg_lm, 23)  #Instead of 98, it is the 104.
# END SOLUTION
```


b) Plot the response and the predictor. Use the `abline()` function to display the least squares regression line. Feel free to use `ggplot2` by using a `geom_smooth` geometry. 

```{r}
# BEGIN SOLUTION
library(ggplot2)
mtcars_plot <- ggplot(mtcars, aes(x=hp, y=mpg)) +
  geom_point() +
  geom_jitter() +
  ylab("MPG") +
  xlab("horsepower") +
  # geom_text(aes(label=name), size=3) +
  geom_smooth(method=lm,   # Add linear regression line
                se=TRUE)  +  # Don't add shaded confidence region
  ylim(0,35) +
  ggtitle('Horsepower and MPG')
mtcars_plot
# END SOLUTION
```


c) Use the `plot()` function to produce diagnostic plots of the least squares regression fit. Comment on any problems you see with the fit. Feel free to use alternatives such as `ggfortify::autoplot`.

```{r}
# BEGIN SOLUTION
# library(ggfortify)
# ggfortify::autoplot(mpg_lm, geom = 'point')
plot(mpg_lm)

# END SOLUTION
```


#### Question 5 (421/521)

This question involves the use of multiple linear regression on the Auto data set. Read the dataset first (go over HW2 for details):

```{r}
# BEGIN SOLUTION
auto <- read.table("https://book.huihoo.com/introduction-to-statistical-learning/Auto.data", header = TRUE)
# END SOLUTION
```


a) Produce a scatterplot matrix which includes all of the variables in the data set.

```{r}
# BEGIN SOLUTION
auto$horsepower <- as.numeric(auto$horsepower)
pairs(auto[ , 1:7],
      col = "blue",                                         # Change color
      pch = 18,                                            # Change shape of points
      labels = c("mpg", "cyl", "displace", "HP", "weight", "accel", "year"),                  # Change labels of diagonal
      main = "Pairs Plot in R") 
# END SOLUTION
```


b) Compute the matrix of correlations between the variables using the function `cor()`. You will need to exclude the name variable, which is qualitative. Also plot the corresponding correlation matrix using `corrplot::corrplot`.

```{r}
# BEGIN SOLUTION
library(corrplot)
M <- cor(auto[, 1:7])
corrplot(M, method = 'number', col = 'black', cl.pos = 'n')
auto$horsepower <- as.integer(auto$horsepower)
corrplot(cor((auto[, c(1, 2, 3, 5, 6, 7, 8)])),        # Correlation matrix
         method = "shade", # Correlation plot method
         type = "full",    # Correlation plot style (also "upper" and "lower")
         diag = TRUE,      # If TRUE (default), adds the diagonal
         tl.col = "black", # Labels color
         bg = "white",     # Background color
         title = "",       # Main title
         col = NULL)       # Color palette
# END SOLUTION
```

c) Use the `lm()` function to perform a multiple linear regression with mpg as the response and all other variables except name as the predictors. Use the `summary()` function to print the results. Use the questions below to comment on the output.

```{r}
# BEGIN SOLUTION

auto_lm <- lm(mpg ~ cylinders + displacement + horsepower + weight + acceleration + year + origin, data = auto)
summary(auto_lm)
# END SOLUTION
```


i. Is there a significant association between the predictors and the response?

> **_Answer:_**  [BEGIN SOLUTION].
Yes. But not for all variables. 

ii. Which predictors appear to have a statistically significant relationship to the response?

> **_Answer:_**  [BEGIN SOLUTION].
Displacement, weight, year, and origin have an effect on MPG.

iii. What does the coefficient for the year variable suggest?

> **_Answer:_**  [BEGIN SOLUTION].
Later car models have improved mpg. 

(d) Use the `plot()` function to produce diagnostic plots of the linear regression fit. Again, feel free to use any other package or function to create these plots. Comment on any problems you see with the fit. Do the residual plots suggest any unusually large outliers? Does the leverage plot identify any observations with unusually high leverage?

```{r}
# BEGIN SOLUTION
plot(auto_lm)

# END SOLUTION
```

> **_Answer:_**  [BEGIN SOLUTION].
Yes, the leverage vs. residual graph does help point out outliers. 


(e) Use the `*` and `:` symbols to fit linear regression models with interaction effects. Please explore a few different models (~3–4 models). Do any interactions appear to be statistically significant in any of the models you examined? Feel free to use stepwise selection if interested (e.g. `MASS::stepAIC`, or check out some basic examples in `caret`: https://github.com/topepo/caret/blob/master/RegressionTests/Code/lmStepAIC.R). Your model exploration does not have to be extensive by any means.

```{r}
# BEGIN SOLUTION
hp_weight_lm <- lm(mpg ~ horsepower*weight, data = auto)
summary(hp_weight_lm)

disp_weight_lm <- lm(mpg ~ displacement*weight, data = auto)
summary(disp_weight_lm)

disp_accel_lm <- lm(mpg ~ displacement*acceleration, data = auto)
summary(disp_accel_lm)

hp_accel_lm <- lm(mpg ~ horsepower*acceleration, data = auto)
summary(hp_accel_lm)
# END SOLUTION
```

> **_Answer:_**  [BEGIN SOLUTION].
When interpreting models with an interaction effect, the acceleration alone is significant ans so is the interaciton of hp and acceleration. The R-squared is .68 for the lm(formula = mpg ~ horsepower * acceleration, data = auto). 

(f) Try a few different transformations of the variables, such as `log(X)`, `sqrt(X)`, $X^2$. Comment on your findings.

```{r}
# BEGIN SOLUTION
sq_year_lm <- lm(mpg ~ year + I(year^2), data = auto)
summary(sq_year_lm)

log_year_lm<- lm(mpg ~ year + log(year), data = auto)
summary(log_year_lm)

sq_accel_lm <- lm(mpg ~ year + I(acceleration^2), data = auto)
summary(sq_accel_lm)
# END SOLUTION
```

> **_Answer:_**  [BEGIN SOLUTION].


#### Question 6 (421/521)

In this exercise you will create some simulated data and will fit simple linear regression models to it. Make sure to use `set.seed(1)` prior to starting part (a) to ensure consistent results.

```{r}
# BEGIN SOLUTION
set.seed(1)

# END SOLUTION
```


a) Using the rnorm() function, create a vector, `x`, containing 100 observations drawn from a $N(0, 1)$ distribution. This represents a feature, `X`.

```{r}
# BEGIN SOLUTION
LO <- 0
UP <- 1
x <- pmax(LO, pmin(UP, rnorm(100)))
# END SOLUTION
```


b) Using the `rnorm()` function, create a vector, `eps`, containing 100 observations drawn from a $N(0, 0.25)$ distribution — a normal distribution with mean zero and variance `0.25`.

```{r}
# BEGIN SOLUTION
library(truncnorm)
var_sq <- .25*.25
eps <- rtruncnorm(n=100, a=0, b=0.25, mean=0, sd=var_sq)
# END SOLUTION
```


c) Using `x` and `eps`, generate a vector `y` according to the model $Y = -1 + 0.5X + ϵ$. What is the length of the vector `y`? What are the values of $β_0$ and $β_1$ in this linear model?

```{r}
# BEGIN SOLUTION
y <- -1 + 0.5*x + eps
y
# END SOLUTION
```

The length of y is 100. The intercept is -1 and the B1 is 0.5.

d) Create a scatterplot displaying the relationship between `x` and `y`. Comment on what you observe.

```{r}
# BEGIN SOLUTION
plot(x,y)

# END SOLUTION
```
The relationship appears fairly linear. Many observations fall where x = 0 or x = 1. 

e) Fit a least squares linear model that relates y and `x`. Comment on the model obtained. How do the estimated coefficients (slope and intercept) compare to the true values used to simulate the data?

```{r}
# BEGIN SOLUTION
xy_lm <- lm(y ~ x)
summary(xy_lm)
# END SOLUTION
```
Adjusted R-sq adjusts for number of predictors. 

f) Display the least squares line on the scatterplot obtained in (d). Draw the estimated regression line on the plot, in a different color. Use the `legend()` command to create an appropriate legend. Feel free to use `ggplot2` or any other framework in R.

```{r}
# BEGIN SOLUTION
# plot(x,y)
# abline(xy_lm) 
# abline(a = -1, b = 0.5, col = "red")  
# legend("topleft",
#   legend="Least Squares Fit",
#   lty=1,
#   lwd=3)

ggp <- ggplot(xy_lm, aes(x, y, color = "red")) +      # Create basic ggplot
  geom_point() + 
  geom_abline(data = xy_lm, aes(slope = 0.5, intercept = -1)) +
  geom_smooth(method=lm,   # Add linear regression line
                se=TRUE) +
  theme(legend.position="left")

ggp 
# END SOLUTION
```


g) Now fit a polynomial regression model that predicts $y$ using $x$ and $x^2$. Is there evidence that the quadratic term improves the model fit? Explain your answer.

```{r}
# BEGIN SOLUTION
x_sq_lm <- lm(y ~ x^2)
summary(x_sq_lm)
# END SOLUTION
```

> **_Answer:_**  [BEGIN SOLUTION].
Not partcularly. The R-squared is similar in the linear and the polymomial model. 

h) Repeat (a)–(f) after modifying the data generation process in such a way that there is less noise in the data. The model analyzed in this section,, should remain the same. You can do this by decreasing the variance of the normal distribution used to generate the error term ϵ in (b). Describe your results.

```{r}
# BEGIN SOLUTION
less_var_sq <- .15*.15
eps2 <- rtruncnorm(n=100, a=0, b=0.25, mean=0, sd=less_var_sq)
y2 <- -1 + 0.5*x + eps2
plot(x,y2)

xy2_lm <- lm(y2 ~ x)
summary(xy2_lm)

ggp2 <- ggplot(xy_lm, aes(x, y2, color = "y2")) +      # Create basic ggplot
  geom_point() + 
  geom_abline(data = xy2_lm, aes(slope = 0.5, intercept = -1)) +
  geom_smooth(method=lm,   # Add linear regression line
                se=TRUE) +
  theme(legend.position="left")

ggp2 
# END SOLUTION
```

> **_Answer:_**  [BEGIN SOLUTION].
The values with less variance fit closer to the abline. 

i) Repeat (a)–(f) after modifying the data generation process in such a way that there is more noise in the data. The model analyzed in this section ($Y = -1 + 0.5X + ϵ$) should remain the same. You can do this by increasing the variance of the normal distribution used to generate the error term $ϵ$ in (b). Describe your results.

```{r}
# BEGIN SOLUTION
more_var_sq <- .5*.5
eps3 <- rtruncnorm(n=100, a=0, b=0.25, mean=0, sd=more_var_sq)
y3 <- -1 + 0.5*x + eps3
plot(x,y3)

xy3_lm <- lm(y3 ~ x)
summary(xy3_lm)

ggp3 <- ggplot(xy_lm, aes(x, y3, color = "y3")) +      # Create basic ggplot
  geom_point() + 
  geom_abline(data = xy3_lm, aes(slope = 0.5, intercept = -1)) +
  geom_smooth(method=lm,   # Add linear regression line
                se=TRUE) +
  theme(legend.position="left")

ggp3
# END SOLUTION
```

> **_Answer:_**  [BEGIN SOLUTION].
Values with more variance had a geom_smooth line further from the abline.

j) What are the confidence intervals for $β_0$ and $β_1$ based on the original data set, the noisier data set, and the less noisy data set? Comment on your results.

```{r}
# BEGIN SOLUTION
confint(xy_lm)
confint(xy2_lm)
confint(xy3_lm)

# END SOLUTION
```

> **_Answer:_**  [BEGIN SOLUTION].
The gap between the conf. intervals for the orginal model was .021. For the model with less noise, the range of values within the confidence intervals was less (0.0068). For the model with more noise, values that bookend the confidence intervals was highest at 0.04.

#### Question 7 (521)
This question focuses on the collinearity problem.
 
(a) Perform the following commands in `R`: 
```{r}
set.seed(1)
x1 <- runif(100)
x2 <- 0.5 * x1 + rnorm(100) / 10
y <- 2 + 2 * x1 + 0.3 * x2 + rnorm(100)
Q7_data <- cbind.data.frame(x1,x2,y)
```

The last line corresponds to creating a linear model in which $y$ is a function of $x_1$ and $x_2$. Write out the form of the linear model. What are the regression coefficients?

```{r}
# BEGIN SOLUTION
multi_lm <- lm(y ~ x1 + x2)
summary(multi_lm)
# END SOLUTION
```


(b) What is the correlation between $x_1$ and $x_2$? Create a scatterplot displaying the relationship between the variables.

```{r}
# BEGIN SOLUTION

# df %>% 
#   gather(which_z, value, -x, -y) %>% 
#   ggplot(aes(x, y, fill = value)) +
#     geom_raster() +
#     facet_wrap(~ which_z)


ggp4 <- ggplot(multi_lm, aes(x1, y, fill = x2)) +      # Create basic ggplot
  geom_point(aes(color = x2)) + 
  # geom_abline(data = multi_lm, aes(slope = 0.5, intercept = -1)) +
  geom_smooth(method=lm,   # Add linear regression line
                se=TRUE) 
  # scale_fill_gradient2(low = "red", mid = "white", high = "green") 

ggp4
# END SOLUTION
```

(c) Using this data, fit a least squares regression to predict $y$ using $x_1$ and $x_2$. Describe the results obtained. What are $β_0$, $β_1$, and $β_2$? How do these relate to the true $β_10$, $β_1$, and $β_2$? Can you reject the null hypothesis $H_0$: $β_1$ = 0? How about the null hypothesis $H_0$: $β_2$ = 0?

```{r}
# BEGIN SOLUTION
multi_lm <- lm(y ~ x1 + x2)
residuals(multi_lm)

summary(multi_lm)
# END SOLUTION
```

> **_Answer:_**  [BEGIN SOLUTION].
You cannot reject the null hypothesis because the predictors are not statistically significant. Also the R squared is very small. 

(d) Now fit a least squares regression to predict $y$ using only $x_1$. Comment on your results. Can you reject the null hypothesis $H_0$: $β_1$ = 0?

```{r}
# BEGIN SOLUTION
x1_lm <- lm(y ~ x1, data = Q7_data)
summary(x1_lm)
# END SOLUTION
```

> **_Answer:_**  [BEGIN SOLUTION].
You cannot reject the null hypothesis, although the R-squared is higher. 

(e) Now fit a least squares regression to predict $y$ using only $x_2$. Comment on your results. Can you reject the null hypothesis $H_0$: $β_1$ = 0?

```{r}
# BEGIN SOLUTION
x2_lm <- lm(y ~ x2, data = Q7_data)
summary(x2_lm)
# END SOLUTION
```

> **_Answer:_**  [BEGIN SOLUTION].
You cannot reject the null hypothesis. 

(f) Do the results obtained in (c)–(e) contradict each other? Explain your answer.

> **_Answer:_**  [BEGIN SOLUTION].
No. From my data, I saw that no single part was significant and the multiple regression model did not show either of the variables to be significant. 

(g) Now suppose we obtain one additional observation, which was unfortunately mismeasured. Re-fit the linear models from (c) to (e) using this new data. What effect does this new observation have on each of the models? In each model, is this observation an outlier? A high-leverage point? Both? Explain your answers.

> **_Answer:_**  [BEGIN SOLUTION].
```{r}

x1 <- append(x1, 2)  
x2 <- append(x2, 2)
y <- append(y, 1)

```
The new observations will be seen as outliers. 
```{r}
multi_lm2 <- lm(y ~ x1 + x2)
summary(multi_lm2)
```
Somehow with the new observations, my model looks significant with a higher R squared of 0.58. The predictors were significant at the .0001 level.
 
#### Question 8 (421/521)

It is claimed that in the case of simple linear regression of $Y$ onto $X$, the $R^2$ statistic is equal to the square of the correlation between $X$ and $Y$. Generate some synthetic data and show that this is the case.

```{r}
# BEGIN SOLUTION
set.seed(2)
new_x <- rtruncnorm(n=100)
new_y <- rtruncnorm(n=100)

new_lm <- lm(new_y ~ new_x)
summary(new_lm)

rev_lm <- lm(new_x ~ new_y)
summary(rev_lm)
# END SOLUTION
```
The new_lm had a multiple R-sq of 0.00764. Yes, the rev_lm also had a multiple R-sq of 0.00764. 

## Extra credit


#### Question 1

Consider the fitted values that result from performing linear regression without an intercept. In this setting, the ith fitted value takes the form 
$$
\hat{y}_i = x_i \hat{\beta}
$$
Where, 

$$
\hat{\beta} = \left( \sum_{i=1}^n x_{i} y_{i} \right) / \left( \sum_{i'=1}^n x_{i'}^2 \right)
$$
Show that we can write

$$
\hat{y}_i = \sum_{i'=1}^n a'_i y'_i
$$

What is $a'_i$?

> **_Answer:_**  [BEGIN SOLUTION].

#### Question 2

Given that,
$$\hat{\beta}_1 = \dfrac{\displaystyle\sum\limits_{i=1}^{n} (x_i-\bar{x})(y_i-\bar{y}) }{\displaystyle \sum\limits_{i=1}^{n}(x_i-\bar{x})^2}\text{,}$$
and $\hat{\beta}_0 = \bar{y}-\hat{\beta}_1\bar{x}$, argue that in the case of simple linear regression, the least squares line always passes through the point ($\bar{x}$, $\bar{y}$).

> **_Answer:_**  [BEGIN SOLUTION].





